{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizers.ipynb  results.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from schedulefree import ScheduleFreeWrapper\n",
    "from mechanic_pytorch import mechanize\n",
    "# from schedulefree import ScheduleFreeWrapper\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "from prodigyopt import Prodigy\n",
    "\n",
    "from custom_optimizers import signSGD, FrankWolfe, MirrorGD\n",
    "\n",
    "\n",
    "from torch import functional as F\n",
    "# from deep import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lmo_1(x, R):\n",
    "    y = x.view(-1)\n",
    "    max_idx = torch.argmax(torch.abs(y))\n",
    "    mask_flat = torch.zeros_like(y)\n",
    "    mask_flat[max_idx] = 1\n",
    "    return -R*mask_flat.view(x.shape)\n",
    "\n",
    "def lmo_2(x, R):\n",
    "    return - R * x / x.norm(p=2)\n",
    "\n",
    "def mirror_entropy_simplex( x, g, R ):\n",
    "    value = torch.exp(-g) * x \n",
    "    return R * value / value.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group R: [tensor(3.5178), tensor(0.1916)]\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(10,1)\n",
    "\n",
    "# model.weight.data = model.weight.data ** 2\n",
    "\n",
    "x = torch.randn(10)  # Входные данные\n",
    "y = torch.tensor(4.0)  # Целевое значение\n",
    "\n",
    "# frankwolfe_optimizer_1 = FrankWolfe(params=model.parameters() , lmo =lmo_1)\n",
    "# frankwolfe_optimizer_2 = FrankWolfe(params=model.parameters() , lmo =lmo_2)\n",
    "# adamw_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-1, weight_decay = 1e-6 )\n",
    "mirrorgd_optimizer = MirrorGD(params=model.parameters(),mirror=mirror_entropy_simplex, lr = 1e-2, scale = 2)\n",
    "\n",
    "num_epochs = 1000\n",
    "# optimizer = adamw_optimizer\n",
    "# optimizer = frankwolfe_optimizer_1\n",
    "# optimizer = frankwolfe_optimizer_2\n",
    "optimizer = mirrorgd_optimizer\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Обнуляем градиенты\n",
    "\n",
    "    output = model(x)\n",
    "\n",
    "    loss = (output - y) ** 2\n",
    "\n",
    "    # print('Loss :',loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3789, 0.0000, 0.0000, 0.3087, 0.0000, 1.8302, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.5178), tensor(0.1916))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data.norm(p=1) , model.bias.data.norm(p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoupled weight decay\n"
     ]
    }
   ],
   "source": [
    "# Optimizers\n",
    "adamw_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay = 1e-6 )\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\n",
    "lion_optimizer = Lion(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "prodigy_optimizer = Prodigy(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "rmsprob_optimizer = optim.RMSprop(model.parameters(), lr=1e-5, alpha=0.99)\n",
    "signsgd_optimizer = signSGD(model.parameters(), lr=1e-5)\n",
    "adagrad_optimizer = optim.Adagrad(model.parameters(), lr=1e-5)\n",
    "lbfgs_optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-5 , history_size= 40, line_search_fn=True)\n",
    "\n",
    "\n",
    "# Wrappers\n",
    "schedulefree_optimizer = ScheduleFreeWrapper( adamw_optimizer, momentum=0.9, weight_decay_at_y=0.1)\n",
    "schedulefree_optimizer.train()\n",
    "mechanize_optimizer = mechanize(torch.optim.AdamW, s_decay=0.0, betas=(0.999,0.999999), store_delta=False)(model.parameters(), lr=1e-5, weight_decay = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4758e+25], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_zero_weight_decay_condition(\n",
    "    module_name: str, module: nn.Module, parameter_name: str, parameter\n",
    "):\n",
    "    del module_name, parameter\n",
    "    return parameter_name.endswith('bias') or isinstance(\n",
    "        module,\n",
    "        (\n",
    "            nn.BatchNorm1d,\n",
    "            nn.LayerNorm,\n",
    "            nn.InstanceNorm1d,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def make_parameter_groups(\n",
    "    model: nn.Module,\n",
    "    custom_groups: dict[tuple[str], dict],  # [(fullnames, options), ...]\n",
    "    zero_weight_decay_condition= default_zero_weight_decay_condition ,\n",
    "):\n",
    "    custom_fullnames = set()\n",
    "    custom_fullnames.update(*custom_groups)\n",
    "    assert sum(map(len, custom_groups)) == len(\n",
    "        custom_fullnames\n",
    "    ), 'Custom parameter groups must not intersect'\n",
    "\n",
    "    parameters_info = {}  # fullname -> (parameter, needs_wd)\n",
    "    for module_name, module in model.named_modules():\n",
    "        for name, parameter in module.named_parameters():\n",
    "            fullname = f'{module_name}.{name}' if module_name else name\n",
    "            parameters_info.setdefault(fullname, (parameter, []))[1].append(\n",
    "                not zero_weight_decay_condition(module_name, module, name, parameter)\n",
    "            )\n",
    "    parameters_info = {k: (v[0], all(v[1])) for k, v in parameters_info.items()}\n",
    "\n",
    "    params_with_wd = {'params': []}\n",
    "    params_without_wd = {'params': [], 'weight_decay': 0.0}\n",
    "    custom_params = {k: {'params': []} | v for k, v in custom_groups.items()}\n",
    "\n",
    "    for fullname, (parameter, needs_wd) in parameters_info.items():\n",
    "        for fullnames, group in custom_params.items():\n",
    "            if fullname in fullnames:\n",
    "                custom_fullnames.remove(fullname)\n",
    "                group['params'].append(parameter)\n",
    "                break\n",
    "        else:\n",
    "            (params_with_wd if needs_wd else params_with_wd)['params'].append(parameter)\n",
    "    assert (\n",
    "        not custom_fullnames\n",
    "    ), f'Some of the custom parameters were not found in the model: {custom_fullnames}'\n",
    "    return [params_with_wd, params_without_wd] + list(custom_params.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_groups = make_parameter_groups(model,{})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = getattr(optim, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.sgd.SGD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.RMSprop( parameter_groups, lr=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ScheduleFreeWrapper( base_optimizer, momentum=0.9, weight_decay_at_y=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Please insert .train() and .eval() calls for the optimizer. See documentation for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tabr_2/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ЛАБКА/TabularDL/schedule_free/schedulefree/wrap_schedulefree.py:115\u001b[0m, in \u001b[0;36mScheduleFreeWrapper.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mode:\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease insert .train() and .eval() calls for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer. See documentation for details\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mException\u001b[0m: Please insert .train() and .eval() calls for the optimizer. See documentation for details"
     ]
    }
   ],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.AdamW(lr = 1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = mechanize(torch.optim.AdamW, s_decay=0.0, betas=(0.999,0.999999), store_delta=False)(model.parameters(), lr=1e-5, weight_decay = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"AdamW\"\n",
    "lr = [\n",
    "    \"_tune_\",\n",
    "    \"loguniform\",\n",
    "    1e-05,\n",
    "    0.001,\n",
    "]\n",
    "weight_decay = [\n",
    "    \"_tune_\",\n",
    "    \"?loguniform\",\n",
    "    0.0,\n",
    "    1e-06,\n",
    "    0.0001,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = mechanize(torch.optim.SGD, s_decay=0.0, betas=(0.999,0.999999), store_delta=False)(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "_project_dir = os.path.dirname(os.getcwd())\n",
    "os.environ['PROJECT_DIR'] = _project_dir\n",
    "sys.path.append(_project_dir)\n",
    "del _project_dir\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional, Union, cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import lib; lib.configure_libraries()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES = [\"task_type\", \"size\", \"n_features\"]\n",
    "_DATASETS_INFO: dict[Path, dict[str, Any]] = {}\n",
    "\n",
    "DATASETS_MAIN = [\n",
    "    # 'churn',\n",
    "    # 'california',\n",
    "    # 'house',\n",
    "    'adult',\n",
    "    # 'diamond',\n",
    "    # 'otto',\n",
    "    # 'higgs-small',\n",
    "    # 'black-friday',\n",
    "    # 'weather-small',\n",
    "    # 'covtype',\n",
    "    # 'microsoft',\n",
    "]\n",
    "# The datasets from the paper \"Why do tree-based models still outperform deep learning on tabular data?\"\n",
    "DATASETS_WHY = [\n",
    "    # 'classif-cat-large-0-covertype',\n",
    "    # 'classif-cat-large-0-road-safety',\n",
    "    # 'classif-cat-medium-0-KDDCup09_upselling',\n",
    "    # 'classif-cat-medium-1-KDDCup09_upselling',\n",
    "    # 'classif-cat-medium-2-KDDCup09_upselling',\n",
    "    # 'classif-cat-medium-0-compass',\n",
    "    # 'classif-cat-medium-1-compass',\n",
    "    # 'classif-cat-medium-0-electricity',\n",
    "    # 'classif-cat-medium-0-rl',\n",
    "    # 'classif-cat-medium-1-rl',\n",
    "    # 'classif-cat-medium-2-rl',\n",
    "    # 'classif-num-large-0-Higgs',\n",
    "    # 'classif-num-large-0-MiniBooNE',\n",
    "    # 'classif-num-large-0-jannis',\n",
    "    # 'classif-num-medium-0-MagicTelescope',\n",
    "    # 'classif-num-medium-1-MagicTelescope',\n",
    "    # 'classif-num-medium-2-MagicTelescope',\n",
    "    # 'classif-num-medium-0-bank-marketing',\n",
    "    # 'classif-num-medium-1-bank-marketing',\n",
    "    # 'classif-num-medium-2-bank-marketing',\n",
    "    # 'regression-num-medium-0-california',\n",
    "    # 'classif-num-medium-0-credit',\n",
    "    # 'classif-num-medium-1-credit',\n",
    "    # 'regression-num-medium-0-house_16H',\n",
    "    # 'classif-num-medium-0-kdd_ipums_la_97-small',\n",
    "    # 'classif-num-medium-1-kdd_ipums_la_97-small',\n",
    "    # 'classif-num-medium-2-kdd_ipums_la_97-small',\n",
    "    # 'classif-num-medium-0-phoneme',\n",
    "    # 'classif-num-medium-1-phoneme',\n",
    "    # 'classif-num-medium-2-phoneme',\n",
    "    # 'classif-num-medium-3-phoneme',\n",
    "    # 'classif-num-medium-4-phoneme',\n",
    "    # 'regression-num-medium-0-pol',\n",
    "    # 'regression-num-medium-1-pol',\n",
    "    # 'classif-num-medium-0-wine',\n",
    "    # 'classif-num-medium-1-wine',\n",
    "    # 'classif-num-medium-2-wine',\n",
    "    # 'classif-num-medium-3-wine',\n",
    "    # 'classif-num-medium-4-wine',\n",
    "    # 'regression-cat-large-0-SGEMM_GPU_kernel_performance',\n",
    "    # 'regression-cat-large-0-black_friday',\n",
    "    # 'regression-cat-large-0-diamonds',\n",
    "    # 'regression-cat-large-0-nyc-taxi-green-dec-2016',\n",
    "    # 'regression-cat-large-0-particulate-matter-ukair-2017',\n",
    "    # 'regression-cat-medium-0-Bike_Sharing_Demand',\n",
    "    # 'regression-cat-medium-1-Bike_Sharing_Demand',\n",
    "    # 'regression-cat-medium-0-Brazilian_houses',\n",
    "    # 'regression-cat-medium-1-Brazilian_houses',\n",
    "    # 'regression-cat-medium-2-Brazilian_houses',\n",
    "    # 'regression-cat-medium-0-Mercedes_Benz_Greener_Manufacturing',\n",
    "    # 'regression-cat-medium-1-Mercedes_Benz_Greener_Manufacturing',\n",
    "    # 'regression-cat-medium-2-Mercedes_Benz_Greener_Manufacturing',\n",
    "    # 'regression-cat-medium-3-Mercedes_Benz_Greener_Manufacturing',\n",
    "    # 'regression-cat-medium-4-Mercedes_Benz_Greener_Manufacturing',\n",
    "    # 'regression-cat-medium-0-OnlineNewsPopularity',\n",
    "    # 'regression-cat-medium-0-analcatdata_supreme',\n",
    "    # 'regression-cat-medium-1-analcatdata_supreme',\n",
    "    # 'regression-cat-medium-2-analcatdata_supreme',\n",
    "    # 'regression-cat-medium-3-analcatdata_supreme',\n",
    "    # 'regression-cat-medium-4-analcatdata_supreme',\n",
    "    # 'regression-cat-medium-0-house_sales',\n",
    "    # 'regression-cat-medium-0-visualizing_soil',\n",
    "    # 'regression-cat-medium-1-visualizing_soil',\n",
    "    # 'regression-cat-medium-2-visualizing_soil',\n",
    "    # 'regression-cat-medium-0-yprop_4_1',\n",
    "    # 'regression-cat-medium-1-yprop_4_1',\n",
    "    # 'regression-cat-medium-2-yprop_4_1',\n",
    "    # 'regression-num-large-0-year',\n",
    "    # 'regression-num-medium-0-Ailerons',\n",
    "    # 'regression-num-medium-1-Ailerons',\n",
    "    # 'regression-num-medium-2-Ailerons',\n",
    "    # 'regression-num-medium-0-MiamiHousing2016',\n",
    "    # 'regression-num-medium-1-MiamiHousing2016',\n",
    "    # 'regression-num-medium-2-MiamiHousing2016',\n",
    "    # 'regression-num-medium-0-cpu_act',\n",
    "    # 'regression-num-medium-1-cpu_act',\n",
    "    # 'regression-num-medium-2-cpu_act',\n",
    "    # 'regression-num-medium-0-elevators',\n",
    "    # 'regression-num-medium-1-elevators',\n",
    "    # 'regression-num-medium-0-fifa',\n",
    "    # 'regression-num-medium-1-fifa',\n",
    "    # 'regression-num-medium-0-houses',\n",
    "    # 'regression-num-medium-0-isolet',\n",
    "    # 'regression-num-medium-1-isolet',\n",
    "    # 'regression-num-medium-2-isolet',\n",
    "    # 'regression-num-medium-0-medical_charges',\n",
    "    # 'regression-num-medium-0-sulfur',\n",
    "    # 'regression-num-medium-1-sulfur',\n",
    "    # 'regression-num-medium-2-sulfur',\n",
    "    # 'regression-num-medium-0-superconduct',\n",
    "    # 'regression-num-medium-0-wine_quality',\n",
    "    # 'regression-num-medium-1-wine_quality',\n",
    "    # 'regression-num-medium-2-wine_quality',\n",
    "]\n",
    "DATASETS_ALL = DATASETS_MAIN + ['weather-big'] + DATASETS_WHY\n",
    "\n",
    "\n",
    "def get_dataset_info(dpath: Union[str, Path]) -> dict:\n",
    "    dpath = lib.get_path(dpath)\n",
    "    if dpath in _DATASETS_INFO:\n",
    "        return _DATASETS_INFO[dpath]\n",
    "\n",
    "    dataset = lib.Dataset.from_dir(dpath, None)\n",
    "    _DATASETS_INFO[dpath] = {\n",
    "        'dataset': (\n",
    "            dpath.name.upper()[:2] if dpath.parent == lib.DATA_DIR and dpath.name in DATASETS_MAIN\n",
    "            else 'WE (full)' if dpath.parent == lib.DATA_DIR and dpath.name == 'weather-big'\n",
    "            else dpath.name\n",
    "        ),\n",
    "        'task_type': dataset.task_type.value,\n",
    "        'size': dataset.size(None),\n",
    "        'n_features': dataset.n_features,   \n",
    "    }\n",
    "    return deepcopy(_DATASETS_INFO[dpath])\n",
    "\n",
    "\n",
    "def load_record(output: Union[str, Path]):\n",
    "    output = lib.get_path(output)\n",
    "    report = lib.load_report(output)\n",
    "    if lib.EXP_DIR in output.parents and '/exp/npt/' in str(output):\n",
    "        # The NPT reports do not follow the required format,\n",
    "        # so we infer the dataset path from the output path.\n",
    "        dpath = ':data/' + list(output.relative_to(lib.EXP_DIR / 'npt').parents)[-2].name\n",
    "    else:\n",
    "        if report[\"function\"] == 'bin.tune.main':\n",
    "            report = report[\"best\"]\n",
    "\n",
    "        if report[\"function\"] == 'bin.ensemble.main':\n",
    "            dpath = report[\"data\"]\n",
    "        else:\n",
    "            data = report[\"config\"][\"data\"]\n",
    "            dpath = data if isinstance(data, str) else data['path']\n",
    "            del data\n",
    "\n",
    "    record = get_dataset_info(dpath)\n",
    "    for part in lib.Part:\n",
    "        if part.value in report[\"metrics\"]:\n",
    "            score = report[\"metrics\"][part.value][\"score\"]\n",
    "            if record['dataset'] == 'HO':\n",
    "                # Prettify the score for \":data/house\".\n",
    "                score /= 10000\n",
    "            record[f\"{part.value}_score\"] = score\n",
    "    return record\n",
    "\n",
    "\n",
    "def _compute_ranks(dataset_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataset_df = dataset_df.sort_values(['test_mean', 'test_std'], ascending=[False, True])\n",
    "    ranks = []\n",
    "    current_score = None\n",
    "    current_std = None\n",
    "    for _, columns in dataset_df.iterrows():\n",
    "        score = columns['test_mean']\n",
    "        std = columns['test_std']\n",
    "        if current_score is None:\n",
    "            ranks.append(1)\n",
    "            current_score = score\n",
    "            current_std = std\n",
    "        elif current_score - score <= current_std:\n",
    "            ranks.append(ranks[-1])\n",
    "        else:\n",
    "            ranks.append(ranks[-1] + 1)\n",
    "            current_score = score\n",
    "            current_std = std\n",
    "    dataset_df['rank'] = ranks\n",
    "    return dataset_df\n",
    "\n",
    "\n",
    "def build_metrics_dataframe(\n",
    "    outputs_info: list[\n",
    "        tuple[\n",
    "            Union[str, Path],  # output path\n",
    "            str,  # key (for example, algorithm name: \"MLP\")\n",
    "            Union[int, str],  # subkey for aggregation (for example, seed: 0)\n",
    "        ]\n",
    "    ],\n",
    "    precision: Optional[int] = 4,\n",
    "):\n",
    "    # >>> Build dataframe.\n",
    "    records = [\n",
    "        load_record(output) | { 'key': key, 'subkey': str(subkey)}\n",
    "        for output, key, subkey in outputs_info\n",
    "        if lib.get_path(output).joinpath('DONE').exists()\n",
    "    ]\n",
    "    if not records:\n",
    "        raise RuntimeError('No records are available')\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    has_train_score = 'train_score' in df.columns\n",
    "\n",
    "    # >>> Aggregate over subkeys.\n",
    "    aggregations = {\n",
    "        'test_mean': (\"test_score\", \"mean\"),\n",
    "        'test_std': (\"test_score\", \"std\"),\n",
    "        'val_mean': (\"val_score\", \"mean\"),\n",
    "        'val_std': (\"val_score\", \"std\"),\n",
    "    }\n",
    "    if has_train_score:\n",
    "        aggregations.update({\n",
    "            'train_mean': (\"train_score\", \"mean\"),\n",
    "            'train_std': (\"train_score\", \"std\"),\n",
    "        })\n",
    "    aggregations['count'] = (\"test_score\", \"count\")\n",
    "    aggregations.update({\n",
    "        x: (x, \"first\")\n",
    "        for x in DATASET_PROPERTIES\n",
    "        if x in df.columns\n",
    "    })\n",
    "    df = df.groupby([\"dataset\", \"key\"]).agg(**aggregations)\n",
    "    df = df.reset_index().fillna(0.0)\n",
    "    df[\"count\"] = df[\"count\"].astype(int)\n",
    "\n",
    "    # >>> Compute ranks.\n",
    "    df = cast(\n",
    "        pd.DataFrame,\n",
    "        df.groupby(['dataset'], group_keys=False).apply(_compute_ranks)\n",
    "    )\n",
    "\n",
    "    # >>> Finalize.\n",
    "    df = df.sort_values(\n",
    "        ['size', 'n_features', 'dataset', 'test_mean'],\n",
    "        ascending=[True, True, True, False],\n",
    "    ).reset_index(drop=True)\n",
    "    df.loc[\n",
    "        df['task_type'] == 'regression',\n",
    "        ['test_mean', 'val_mean'] + ['train_mean'] * int(has_train_score)\n",
    "    ] *= -1\n",
    "    if precision is not None:\n",
    "        float_columns = [\n",
    "            'test_mean', 'test_std',\n",
    "            'val_mean', 'val_std',\n",
    "        ] + ['train_mean', 'train_std'] * int(has_train_score)\n",
    "        df[float_columns] = df[float_columns].round(precision)\n",
    "    df = df.set_index([\"dataset\"] + DATASET_PROPERTIES + [\"key\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_ranks(metrics_df: pd.DataFrame, nans: bool) -> pd.DataFrame:\n",
    "    df = metrics_df\n",
    "    df = df.reset_index().pivot(index='key', columns='dataset', values='rank')\n",
    "    if not nans:\n",
    "        df = df.dropna(axis='columns')\n",
    "    columns = df.columns.tolist()\n",
    "    df[\"avg\"] = df.mean(1)\n",
    "    df[\"std\"] = df.std(1)\n",
    "    df.insert(0, \"avg\", df.pop(\"avg\").round(1))\n",
    "    df.insert(1, \"std\", df.pop(\"std\").round(1))\n",
    "    df = df.sort_values(\"avg\")\n",
    "    df = df[['avg', 'std'] + columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the next cell\n",
    "- comment/uncomment `N_SEEDS += 15` to show/hide results for single models\n",
    "- comment/uncomment `N_ENSEMBLES += 3` to show/hide results for ensembles\n",
    "- in the `for dataset in datasets` loop:\n",
    "    - comment/uncomment the `add(...)` lines to show/hide results for various algorithms\n",
    "    - in particular, uncomment `add(f':exp/mlp/{dataset}/0-reproduce', 'MLP (reproduce)')` to complete the tutorial from `README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>AD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(E) CatBoost</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) LightGBM</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) MLP-PLR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) TabR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) XGBoost</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabR</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP-PLR</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) MLP</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset       avg  std  AD\n",
       "key                       \n",
       "(E) CatBoost  1.0  0.0   1\n",
       "(E) LightGBM  1.0  0.0   1\n",
       "(E) MLP-PLR   1.0  0.0   1\n",
       "(E) TabR      1.0  0.0   1\n",
       "(E) XGBoost   1.0  0.0   1\n",
       "CatBoost      1.0  0.0   1\n",
       "XGBoost       1.0  0.0   1\n",
       "LightGBM      2.0  0.0   2\n",
       "TabR          3.0  0.0   3\n",
       "MLP-PLR       4.0  0.0   4\n",
       "(E) MLP       5.0  0.0   5\n",
       "MLP           5.0  0.0   5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_mean</th>\n",
       "      <th>test_std</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>val_std</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>train_std</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>task_type</th>\n",
       "      <th>size</th>\n",
       "      <th>n_features</th>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">AD</th>\n",
       "      <th rowspan=\"12\" valign=\"top\">binclass</th>\n",
       "      <th rowspan=\"12\" valign=\"top\">48842</th>\n",
       "      <th rowspan=\"12\" valign=\"top\">14</th>\n",
       "      <th>(E) MLP-PLR</th>\n",
       "      <td>0.8726</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.8719</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) XGBoost</th>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.8709</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.9144</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) CatBoost</th>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) TabR</th>\n",
       "      <td>0.8722</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.8722</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8739</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9210</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) LightGBM</th>\n",
       "      <td>0.8721</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.8721</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.8711</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabR</th>\n",
       "      <td>0.8706</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP-PLR</th>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(E) MLP</th>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.8539</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 test_mean  test_std  \\\n",
       "dataset task_type size  n_features key                                 \n",
       "AD      binclass  48842 14         (E) MLP-PLR      0.8726    0.0008   \n",
       "                                   (E) XGBoost      0.8723    0.0002   \n",
       "                                   (E) CatBoost     0.8723    0.0007   \n",
       "                                   (E) TabR         0.8722    0.0005   \n",
       "                                   XGBoost          0.8722    0.0000   \n",
       "                                   (E) LightGBM     0.8721    0.0004   \n",
       "                                   CatBoost         0.8721    0.0000   \n",
       "                                   LightGBM         0.8711    0.0000   \n",
       "                                   TabR             0.8706    0.0000   \n",
       "                                   MLP-PLR          0.8694    0.0000   \n",
       "                                   (E) MLP          0.8540    0.0002   \n",
       "                                   MLP              0.8539    0.0000   \n",
       "\n",
       "                                                 val_mean  val_std  \\\n",
       "dataset task_type size  n_features key                               \n",
       "AD      binclass  48842 14         (E) MLP-PLR     0.8719   0.0005   \n",
       "                                   (E) XGBoost     0.8709   0.0012   \n",
       "                                   (E) CatBoost    0.8724   0.0005   \n",
       "                                   (E) TabR        0.8748   0.0012   \n",
       "                                   XGBoost         0.8739   0.0000   \n",
       "                                   (E) LightGBM    0.8729   0.0006   \n",
       "                                   CatBoost        0.8750   0.0000   \n",
       "                                   LightGBM        0.8753   0.0000   \n",
       "                                   TabR            0.8756   0.0000   \n",
       "                                   MLP-PLR         0.8736   0.0000   \n",
       "                                   (E) MLP         0.8579   0.0014   \n",
       "                                   MLP             0.8592   0.0000   \n",
       "\n",
       "                                                 train_mean  train_std  count  \\\n",
       "dataset task_type size  n_features key                                          \n",
       "AD      binclass  48842 14         (E) MLP-PLR       0.8849     0.0054      3   \n",
       "                                   (E) XGBoost       0.9144     0.0042      3   \n",
       "                                   (E) CatBoost      0.8976     0.0050      3   \n",
       "                                   (E) TabR          0.8856     0.0007      3   \n",
       "                                   XGBoost           0.9210     0.0000      1   \n",
       "                                   (E) LightGBM      0.9065     0.0022      3   \n",
       "                                   CatBoost          0.8968     0.0000      1   \n",
       "                                   LightGBM          0.9057     0.0000      1   \n",
       "                                   TabR              0.8807     0.0000      1   \n",
       "                                   MLP-PLR           0.8968     0.0000      1   \n",
       "                                   (E) MLP           0.8806     0.0024      3   \n",
       "                                   MLP               0.8801     0.0000      1   \n",
       "\n",
       "                                                 rank  \n",
       "dataset task_type size  n_features key                 \n",
       "AD      binclass  48842 14         (E) MLP-PLR      1  \n",
       "                                   (E) XGBoost      1  \n",
       "                                   (E) CatBoost     1  \n",
       "                                   (E) TabR         1  \n",
       "                                   XGBoost          1  \n",
       "                                   (E) LightGBM     1  \n",
       "                                   CatBoost         1  \n",
       "                                   LightGBM         2  \n",
       "                                   TabR             3  \n",
       "                                   MLP-PLR          4  \n",
       "                                   (E) MLP          5  \n",
       "                                   MLP              5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_SEEDS = 1\n",
    "N_ENSEMBLES = 1\n",
    "N_ENSEMBLES += 15\n",
    "\n",
    "# See the comments in build_metrics_dataframe to learn about outputs_info.\n",
    "outputs_info = []\n",
    "def add(location: str, name: Optional[str] = None, sep: str = '-'):\n",
    "    if name is None:\n",
    "        assert location.startswith(':exp/')\n",
    "        # location example: \":exp/mlp/california/0\"\n",
    "        _exp_prefix, alg, *_dataset, tag = location.split('/')\n",
    "        name = f'{alg}[{tag}]'\n",
    "    for seed in range(N_SEEDS):\n",
    "        outputs_info.append((location + f'{sep}evaluation/{seed}', name, seed))\n",
    "    for ensemble_i in range(N_ENSEMBLES):\n",
    "        outputs_info.append((location + f'{sep}ensemble{sep}5/{ensemble_i}', '(E) ' + name, ensemble_i))\n",
    "\n",
    "datasets = DATASETS_MAIN\n",
    "for dataset in datasets:\n",
    "    if dataset in DATASETS_WHY:\n",
    "        dataset = 'why/' + dataset\n",
    "\n",
    "    # >>> Tutorial from README.md\n",
    "    # add(f':exp/mlp/{dataset}/0-reproduce', 'MLP (reproduce)')\n",
    "\n",
    "    # >>> Retrieval-augmented baselines\n",
    "    # add(f':exp/knn/{dataset}/0', 'kNN')\n",
    "\n",
    "    # dnnr_tag = 'ohe' if dataset in [BLACK_FRIDAY, DIAMOND] else 'loo'\n",
    "    # add(f':exp/dnnr/{dataset}/{dnnr_tag}', 'DNNR')\n",
    "\n",
    "    # add(f':exp/anp/{dataset}/0', 'ANP')\n",
    "    # add(f':exp/dkl/{dataset}/0', 'DKL')\n",
    "\n",
    "    # npt_tag = {\n",
    "    #     'churn': 0,\n",
    "    #     'california': 0,\n",
    "    #     'house': 0,\n",
    "    #     'adult': 0,\n",
    "    #     'diamond': 2,\n",
    "    #     'otto': 1,\n",
    "    #     'higgs-small': 2,\n",
    "    #     'black-friday': 2,\n",
    "    #     'covtype': 3,\n",
    "    #     'weather-small': 1,\n",
    "    #     'microsoft': 1,\n",
    "    # }[dataset]\n",
    "    # add(f':exp/npt/{dataset}/{npt_tag}', 'NPT')\n",
    "\n",
    "    # saint_tag = 'default' if dataset in ['weather-small', 'covtype', 'microsoft'] else '2'\n",
    "    # add(f':exp/saint/{dataset}/{saint_tag}', 'SAINT')\n",
    "\n",
    "    # >>> Parametric DL baselines\n",
    "    add(f':exp/mlp/{dataset}/0', 'MLP')\n",
    "    # add(f':exp/mlp/{dataset}/lr', 'MLP-LR')\n",
    "    # add(f':exp/mlp/{dataset}/plr-lite', 'MLP-PLR(lite)')\n",
    "    add(f':exp/mlp/{dataset}/plr', 'MLP-PLR')\n",
    "\n",
    "    # >>> GBDT\n",
    "    # add(f':exp/xgboost_/{dataset}/default2', 'XGBoost (default)')\n",
    "    # add(f':exp/lightgbm_/{dataset}/default2', 'LightGBM (default)')\n",
    "    # add(f':exp/catboost_/{dataset}/default2', 'CatBoost (default)')\n",
    "    add(f':exp/xgboost_/{dataset}/2', 'XGBoost')\n",
    "    add(f':exp/lightgbm_/{dataset}/2', 'LightGBM')\n",
    "    add(f':exp/catboost_/{dataset}/2', 'CatBoost')\n",
    "\n",
    "    # >>> The model\n",
    "    model = 'TabR'\n",
    "    modeldir = model.lower()\n",
    "    # add(f':exp/{modeldir}/{dataset}/default', f'{model}-S (default)')\n",
    "    # add(f':exp/{modeldir}/{dataset}/0', f'{model}-S')\n",
    "    model_tag = \"2-lr\" if dataset in ['weather-small', 'covtype', 'microsoft'] else \"2-plr-lite\"\n",
    "    add(f':exp/{modeldir}/{dataset}/{model_tag}', f'{model}')\n",
    "\n",
    "    # >>> Ablation study\n",
    "    for tag, name in [\n",
    "        # ('dp-qk-v-self-scaled', 'Step-0'),\n",
    "        # ('dp-qk-yv-self-scaled', 'Step-1'),\n",
    "        # ('l2-k-yv-self-scaled', 'Step-2'),\n",
    "        # ('l2-k-yt-self-scaled', 'Step-3'),\n",
    "    ]:\n",
    "        add(f':exp/{modeldir}_design/{dataset}/{tag}', f'(design) {name}')\n",
    "\n",
    "    # >>> Context freeze\n",
    "    for freeze_after_n_epochs in [\n",
    "        # 0,\n",
    "        # 1,\n",
    "        # 2,\n",
    "        # 4,\n",
    "        # 5,\n",
    "        # 8,\n",
    "    ]:\n",
    "        add(f':exp/{modeldir}_scaling/{dataset}/default-freeze-{freeze_after_n_epochs}', f'{model}-freeze-{freeze_after_n_epochs}')\n",
    "\n",
    "metrics_df = build_metrics_dataframe(outputs_info)\n",
    "# Drop details about datasets to save screen space.\n",
    "# while len(metrics_df.index.levels) > 2:\n",
    "#     metrics_df.index = metrics_df.index.droplevel(1)\n",
    "ranks_df = summarize_ranks(metrics_df, nans=True)\n",
    "print('Ranks:')\n",
    "display(ranks_df)\n",
    "print('\\nMetrics:')\n",
    "display(metrics_df)\n",
    "# metrics_df.to_html('metrics.html')\n",
    "# ranks_df.to_html('metrics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabr_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
